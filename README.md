# PyTorchSeminar

Добро пожаловать в **PyTorchSeminar** — репозиторий, содержащий Jupyter Notebook (`ML-5.ipynb`), посвященный изучению основ работы с библиотекой **PyTorch** для машинного обучения. Этот проект демонстрирует базовые операции с тензорами, автоматическое вычисление градиентов, а также обучение нейронной сети для классификации изображений из датасета MNIST.

## Обзор

Репозиторий включает Jupyter Notebook (`ML-5.ipynb`), который охватывает следующие темы:
- Сравнение операций с массивами в **NumPy** и **PyTorch**.
- Создание и манипуляция тензорами в **PyTorch** (сложение, матричное умножение, reshape, broadcasting и др.).
- Автоматическое вычисление производных с использованием `requires_grad`.
- Работа с датасетом MNIST: загрузка, визуализация и предобработка данных.
- Построение и обучение нейронной сети для классификации цифр (полносвязная сеть с одним скрытым слоем).
- Использование GPU/CPU для ускорения вычислений.
- Оценка модели на валидационной выборке (потери и точность).

Ноутбук содержит подробные комментарии, код для визуализации и анализа данных, а также результаты обучения модели.

## Набор данных

- **MNIST**:
  - Датасет рукописных цифр (0–9), загружаемый через `torchvision.datasets.MNIST`.
  - Размер изображений: 28x28 пикселей (черно-белые).
  - Количество классов: 10 (цифры от 0 до 9).
  - Данные разделены на обучающую (60,000 изображений) и тестовую (10,000 изображений) выборки.

## Требования

Для работы с ноутбуком необходимы следующие библиотеки Python:
- `numpy`
- `pandas`
- `matplotlib`
- `torch`
- `torchvision`
- `tqdm`

Установите их с помощью команды:
```bash
pip install numpy pandas matplotlib torch torchvision tqdm
```

## Структура репозитория

- `ML-5.ipynb`: Основной Jupyter Notebook с примерами операций PyTorch и обучением модели.
- `README.md`: Этот файл с описанием проекта.

## Использование

1. Склонируйте репозиторий:
   ```bash
   git clone https://github.com/<your-username>/PyTorchSeminar.git
   ```
2. Перейдите в папку репозитория:
   ```bash
   cd PyTorchSeminar
   ```
3. Запустите Jupyter Notebook:
   ```bash
   jupyter notebook ML-5.ipynb
   ```
4. Следуйте инструкциям в ноутбуке для выполнения операций с тензорами, обучения модели и оценки результатов.

## Основные разделы ноутбука

1. **Сравнение NumPy и PyTorch**:
   - Примеры операций с массивами/тензорами: создание, сложение, матричное умножение, вычисление среднего, reshape.
   - Пример задачи: вычисление суммы квадратов чисел от 1 до 10,000 в NumPy и PyTorch (результат: `333383335000`).
   - Таблица соответствия операций NumPy и PyTorch (например, `x.reshape` → `x.view`, `x.sum(axis=-1)` → `x.sum(dim=-1)`).

2. **Создание и манипуляция тензорами в PyTorch**:
   - Создание тензоров: `torch.empty`, `torch.zeros`, `torch.tensor`, `torch.randn_like`.
   - Операции: сложение (`torch.add`), матричное умножение (`torch.matmul`, `x.mm`), поэлементное умножение, вычисление среднего.
   - Изменение размерности: `unsqueeze`, `squeeze`, `view`, `reshape`.
   - Преобразование между NumPy и PyTorch: `torch.from_numpy`, `x.numpy()`.

3. **Автоматическое вычисление производных**:
   - Примеры вычисления градиентов с `requires_grad=True`:
     - Для функции `y = 10x - 1` производная `dy/dx = 10`.
     - Для функции `y = sin(x)` производная `dy/dx = cos(x)` (например, при `x = π` градиент ≈ `-1`).
     - Для функции `y = log(x)` производная `dy/dx = 1/x` (например, при `x = 8` градиент = `0.125`).

4. **Работа с датасетом MNIST**:
   - Загрузка данных с помощью `torchvision.datasets.MNIST` и создание загрузчиков данных (`DataLoader`) с размером батча 4.
   - Визуализация примеров изображений из датасета.
   - Предобработка: изображения преобразуются в тензоры и нормализуются.

5. **Построение и обучение нейронной сети**:
   - Модель: полносвязная сеть (`nn.Sequential`):
     - `nn.Flatten`: преобразование изображений 28x28 в вектор (784).
     - `nn.Linear(784, 128)`: первый слой (128 нейронов).
     - `nn.ReLU`: активация.
     - `nn.Linear(128, 10)`: выходной слой (10 классов).
   - Оптимизатор: SGD с шагом обучения `lr=0.01`.
   - Функция потерь: кросс-энтропия (`F.cross_entropy`).
   - Обучение на 5 эпохах с выводом потерь и точности на валидационной выборке.

6. **Использование GPU/CPU**:
   - Проверка доступности GPU с помощью `torch.cuda.is_available()`.
   - Перенос модели и данных на устройство (`model.to(device)`, `x.to(device)`).
   - Обеспечение совместимости тензоров на одном устройстве для вычислений.

7. **Результаты обучения**:
   - После 5 эпох обучения на MNIST:
     - Эпоха 0: потери = `0.2227`, точность = `0.9364`.
     - Эпоха 1: потери = `0.1540`, точность = `0.9557`.
     - Эпоха 2: потери = `0.1134`, точность = `0.9674`.
     - Эпоха 3: потери = `0.0942`, точность = `0.9715`.
     - Эпоха 4: потери = `0.0862`, точность = `0.9741`.
   - Модель демонстрирует стабильное снижение потерь и рост точности.

## Пример использования

```python
# Создание тензора и вычисление градиента
x = torch.tensor(5.0, requires_grad=True)
y = 10 * x - 1
y.backward()
print(f"x.grad = {x.grad}")  # Вывод: x.grad = 10.0

# Загрузка MNIST и обучение модели
train_dataset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transform)
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)

model = nn.Sequential(nn.Flatten(), nn.Linear(28*28, 128), nn.ReLU(), nn.Linear(128, 10))
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(5):
    for x_train, y_train in train_dataloader:
        y_pred = model(x_train)
        loss = F.cross_entropy(y_pred, y_train)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```
